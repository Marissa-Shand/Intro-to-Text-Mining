---
title: "Intro to Text Mining"
author: "Pantea Ferdosian, Kevin Hoffman, Luke Moles, Marissa Shand"
output:
 html_document:
   toc: TRUE
   theme: united
   toc_depth: 3
   number_sections: TRUE
   df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text Mining Motivation

#### **Need for Text Mining**:

The amount of data that we produce every day is truly astonishing. With the evolution of communication through social media, we generate tons and tons of data. Out of all of this generated data, only around $20\%$ are Structured and well formatted. The remaining of the data is Unstructured Data For instance, the bulk of emails we send, the text messages and the comments in social media are examples of Unstructured Data.
These Unstructured data may be analyzed and mined in order to extract useful information. 


#### What is "Text Mining" and where is it used? 

Text Mining is basically the process of extracting meaningful information from natural language text. 

A few examples of how we use text mining every day:

* Auto-complete feature
* Spam Detection
* Predictive Typing
* Spell Checker
    
####  Terminologies In Text Mining:

**Tokenization: ** The process of splitting the whole data (corpus) into smaller chunks is known as tokenization. (i.e. sentences into words)

**Remove Stop Words:** Stop words that are words that are very commonly used such as "although", "really", "from", etc. By removing such words in a language, we may focus on the important words instead.

**Document Term Matrix or DTM:** A matrix containing the terms that appear in a collection of documents and the frequency of the terms, where rows represent the documents and columns represent the words (terms). 


# Working with text data

"A **token** is a meaningful unit of text, such as wa word, that we are interested in using for analysis, and tokenization is the process of splitting test into tokens" [1]. Can be a word, n-gram, sentence or paragraph.

## Packages we will be using 

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(dplyr)
```

## Exploring Project Gutenberg

"Project Gutenberg is an online library of free eBooks. Project Gutenberg was the first provider of free electronic books, or eBooks." [2]

```{r}
## Find books by Charles Dickens
dickens <- gutenberg_works(author == 'Dickens, Charles')
dim(dickens)
head(dickens)

## We will be working with A Tale of Two Cities, which has id 98
two_cities <- gutenberg_download(98)
head(two_cities)

## There are three books that make up this book
## Get book number
two_cities <- two.cities %>% mutate(book = cumsum(str_detect(text, regex("^Book the"))),
                                    linenumber = row_number())

## For each book get the linenumber in the book, and the chapter
## Roman numerals: https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html
two_cities <- two_cities %>% group_by(book) %>% 
   mutate(book_linenumber = row_number(),
          chapter = cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})[.]")))) %>%
   ungroup()

## Convert to tidy text
tidy_two_cities <- two_cities %>% unnest_tokens(word, text)
head(tidy_two_cities)
```
## Stop words

```{r}
## Error arose: https://stackoverflow.com/questions/9221310/r-debugging-only-0s-may-be-mixed-with-negative-subscripts
data(stop_words)
stop_words

tidy_two_cities %>% anti_join(stop_words, by = c("word" = "word"))
```


# Sentiment Analysis

## Sentiment Lexicons 

tidytext provides three general purpose lexicons:

1. AFINN

2. Bing

3. NRC

# TF-IDF

**How should quantify what a document is about?**


* TF term is short for *term frequency* which indicates how frequently a word appears in a document. The document would most likely include unimportant words which we classified earlier as *stop words*, but removing the stop words would not be an efficient approach in this case because some of these words might be more important in some documents than others. 

* IDF term or *inverse document frequency* defined as :$idf(term) = ln(\frac{n_{documenta}}{n_{documents\ containing\ term}})$. This method adjust the weights and balances the usage of words in documents. This implies that it decreases the weight for widely used words and increases the weight for words that did not appear a lot.

* TF-IDF is short for *term frequencyâ€“inverse document frequency*, which is a statistical measure that indicates how important a word is to a document in a collection. This is obtained by multiplying the 2 mentioned quantities, which would indicate "the frequency of a term adjusted for how rarely it is used".

## Top 10 words with highest tf-idf values in Charles Dicken's A Tale of Two Cities (By chapters)



```{r}

## Find books by Charles Dickens
dickens <- gutenberg_works(author == 'Dickens, Charles')
dim(dickens)
head(dickens)
## We will be working with A Tale of Two Cities, which has id 98
two_cities <- gutenberg_download(98)
head(two_cities)
## There are three books that make up this book
## Get book number
two_cities <- two_cities %>% mutate(book = cumsum(str_detect(text, regex("^Book the"))),
                                    linenumber = row_number())
## For each book get the linenumber in the book, and the chapter
## Roman numerals: https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html
two_cities <- two_cities %>% group_by(book) %>% 
   mutate(book_linenumber = row_number(),
          chapter = cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})[.]")))) %>%
   ungroup()
## Convert to tidy text
tidy_two_cities <- two_cities %>% unnest_tokens(word, text)
head(tidy_two_cities)
```


```{r}
tidy_two_cities %>%
    count(chapter, word, sort = TRUE) %>%
    bind_tf_idf(word, chapter, n) %>%
    arrange(-tf_idf) %>%
    group_by(chapter) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, tf_idf)) %>%
    ggplot(aes(word, tf_idf, fill = chapter)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ chapter, scales = "free") +
    coord_flip()
```

As shown below, the $tf$-$idf$ values are not different after we remove the stop words because the $idf$ values of our stop words are very small, as they appear in all chapters.


## After removing Stop words

```{r}
## Error arose: https://stackoverflow.com/questions/9221310/r-debugging-only-0s-may-be-mixed-with-negative-subscripts
data(stop_words)
stop_words
tidy_two_cities %>% anti_join(stop_words, by = c("word" = "word"))
```

```{r}
tidy_two_cities %>%
    count(chapter, word, sort = TRUE) %>%
    bind_tf_idf(word, chapter, n) %>%
    arrange(-tf_idf) %>%
    group_by(chapter) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, tf_idf)) %>%
    ggplot(aes(word, tf_idf, fill = chapter)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ chapter, scales = "free") +
    coord_flip()
```

# Relationships between words: n-grams and correlations

# Topic Modeling: Latent Direchlet Allocation

Topic modeling is an unsupervised method of clustering words from documents into topics. It is an unsupervised method because we do not have true labels for the topics, but our goal is to discover latent or hidden patterns in documents. It is similar idea to clustering numeric data. Topic modeling is great for finding patterns in large collections of documents. For example, if you were a paralegal and had a large batch of emails from discovery, you could use topic modeling to try to identify the topics people were writing about. Another example is if you were a researcher looking into what people tweeted about on Election day, you could use topic modeling to identify the themes of the tweets.

One of the most popular methods for topic modeling is Latent Dirichlet Allocation. Latent Dirichlet Allocation is a probabilistic generative model where each document is a collection of topics and each topic is a collection of words [5]. For exaple, a document could be comprised of 60% of one topic and 40% of another topic. If a topic was made of 10% of the word apple, 5% of the word orange, etc, the topic could be fruit. Below is a graphical model of LDA:

![LDA model](LDA model.png)

$\alpha$ is a hyperparameter on the Dirichlet distribution, $\theta$ of topics in a document.

$\beta$ is a  hyperparameter for the Dirichlet distribution of words in a particular document.

$M$ represents the number of topics and $N$ represents the numbers

$z$ is the topics and $w$ are the words. $w$ is shaded because it is a known variable.

First you choose the values of your hyperparameters and $M$ the number of topics. For each of the $N$ words, we want to choose a topic $z_n$ from the multinomial ($\Theta$) distribution, which represdents the distribution of topics in each document. Then we want to choose a word $w_n$ from the conditional probability $p(w_n|z_n,\ebta)$ which is the probability of the word given the document. After going through each word and assigning the proprtion of words to topics and topics and documents, we repeat this process a specified number of times. Calculating the posterior is intractible, so we use Gibbes sampling to approximate the posterior distribution.

To show how LDA can work in practice, let's consider an example. Say you have 20 book titles from the top 10 authors from the Gutenburg and want to know how many authors wrote the books and which books were written by the same author. We can use topic modeling to see how to cluster the books presumably into topics which represent the authors of the books.


First we'll create an array of the author names.
```{r}
# Set of popular authors
authors.popular <-
   c(
      "Dickens, Charles",
      "Austen, Jane",
      "Shelley, Mary Wollstonecraft",
      "Twain, Mark",
      "Doyle, Arthur Conan",
      "Wilde, Oscar",
      "Leech, John",
      "Hawthorne, Nathaniel",
      "Stevenson, Robert Louis",
      "Carroll, Lewis"
   )
```


Then we will get all books by these authors from the Gutenberg project. We will limit our selection to works written in English and ignore collection of works.

```{r}
books.authors.popular <- gutenberg_metadata %>%
   filter(
      author %in% authors.popular,
      language == "en", # Only English works
      !str_detect(title, "Works"),  # Ignore collections of works
      !str_detect(title, "Part"),
      !str_detect(title, "Volume"),
      !str_detect(title, "Chapters"),
      has_text,
      !str_detect(rights, "Copyright")
   ) %>%
   distinct(title, .keep_all = TRUE) %>% # Remove duplicate titles
   select(gutenberg_id, title, author)
```

We will select a random sample of 20 books and download them.

```{r}
set.seed(750)
books.selection <- books.authors.popular %>% sample_n(20)

# Download the 20 books
books.list <-
   books.selection$gutenberg_id %>% gutenberg_download(meta_fields = "title")
```

Now we have the text of all 20 books in a dataframe with the id and title. We will need to remove blank rows of the text and convert to tidytext.

```{r}
books.text <- books.list %>%
   filter(text != '') %>% # Remove blank lines
   select(-gutenberg_id) %>% # Drop the id
   group_by(title)  %>%
   unite(document, title)

words.by.book <- books.text %>%
   unnest_tokens(word, text)
```

Now we need to get the count of words so we can turn our data into a Document Term Matrix. We will also remove stop words. 

```{r}
word.counts <- words.by.book %>%
   anti_join(stop_words) %>%
   filter(
   !str_detect(word, "[0-9]"),
   !str_detect(word, "^_"),
   ) %>%
   count(document, word, sort = TRUE) %>%
   ungroup()

# Create a Document Term Matrix
books.dtm <- word.counts %>%
   cast_dtm(document, word, n)
```


We can run LDA with the LDA function from the topicmodels package. We will also have to select a k for the number of topics (authors). Let's try 5.

```{r}
library(topicmodels)
books.lda <- LDA(books.dtm, k = 5, control = list(seed = 555))
```

First, we can look at the probabilities of each word belonging to each topic. Convert the lda results to a tidy dataframe. This will have a row for each topic and term and its probability. Let's convert this into showing the 7 most likely words in each topic and graph it.
```{r}
books.topics <- tidy(books.lda, matrix = "beta")
words.top <- books.topics %>% 
   group_by(topic) %>%
  top_n(7, beta) %>%
  ungroup() %>%
   mutate(term = reorder_within(term, beta, topic))

ggplot(words.top, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 scale_y_reordered()

```

Let's look at some uncommon words or names in topic 1 such as "henry" and "stevenson"

```{r}
word.counts %>% filter(word == "henry")

word.counts %>% filter(word=="stevenson")
```
"henry" appears most often in "The Master of Ballantrae: A Winter's Tale" and "stevenson" appears most often in "Essays of Robert Louis Stevenson". We don't know this now, but both of these books are by Robert Louis Stevenson, a good sign. However we don't know how many authors we have so ideally we can run this LDA model for every possible number of topics (authors) 1 to 10 and have some evaluation metric to determine which is the best "fit". We can use the ldatuning package to do this for us.

```{r}
library(ldatuning)
topics.results <- FindTopicsNumber(
      books.dtm,
      topics = seq(1,10),
      metrics = "Arun2010",
      method = "Gibbs",
)

# Printresults
topics.results %>% arrange(Arun2010)

```

We want to sleect k that is the smallest as the Arun2010 metric measure the K-L divergence and this value is higher for non-optimal values of k [7]. The lowest k is 8. We can run our LDA model again with k-8 and look at the top word probabilities again.

```{r}
books.lda.optimal <- LDA(books.dtm, k = 8, control = list(seed = 555))

books.topics.optimal <- tidy(books.lda.optimal, matrix = "beta")
words.top.optimal <- books.topics.optimal %>% 
   group_by(topic) %>%
  top_n(7, beta) %>%
  ungroup() %>%
   mutate(term = reorder_within(term, beta, topic))

ggplot(words.top.optimal, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 scale_y_reordered()

```

We can also look at the composition of topics in each document. Since each document (book) is written by a different author (and not a mixture of authors), we expect that one topic for each book should stand out above the rest.

```{r}
books.docs.optimal <- tidy(books.lda.optimal, matrix = "gamma")
books.docs.optimal

books.docs.optimal %>%
  mutate(document= reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma))
```

Let's compare the number of unique authors in our set to the k we selected.

```{r}

books.selection %>% distinct(author) %>% count()

```


# TODO:
# - Revise LDA section
# - Update graphics to make new ones rather than following tidy text
# - Finish the example lda. Discuss if it grouped books into comon authors

```


# References

1. [Text Mining with R](https://www.tidytextmining.com/index.html)

2. [Project Gutenberg](https://www.gutenberg.org/)

3. [Roman Numerals with Regex](https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html)

4. [Sentiment Datasets](https://www.datacamp.com/community/tutorials/sentiment-analysis-R)

5. [Latent Dirichlet Allocation](http://www.cse.cuhk.edu.hk/irwin.king/_media/presentations/latent_dirichlet_allocation.pdf)

6. [TF-IDF] (https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html) 

7. [On Finding the Natural Number of Topics with Latent Dirichlet Allocation] (https://doi.org/10.1007/978-3-642-13657-3_43)
